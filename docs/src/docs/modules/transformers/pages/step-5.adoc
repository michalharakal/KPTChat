= Attention

image::transformer_self-attention_visualization.png[]


Selbst-Attention ist eine Sequenz-zu-Sequenz-Operation: Eine Sequenz von Vektoren geht hinein und eine Sequenz von Vektoren kommt heraus. Nennen wir die Eingabevektoren ğ±1,ğ±2,â€¦,ğ±t und die entsprechenden Ausgabevektoren ğ²1,ğ²2,â€¦,ğ²t. Die Vektoren haben alle die Dimension k.

In Transformern ermÃ¶glicht die Attention-Mechanik einem Modell, die Wichtigkeit verschiedener Teile einer Eingabesequenz (wie WÃ¶rter in einem Satz) zu gewichten, wodurch es relevante Informationen hervorheben kann. Dies geschieht durch Berechnung von Aufmerksamkeits-Scores zwischen allen Paaren von Eingabeelementen, wodurch das Modell lernen kann, ZusammenhÃ¤nge zwischen ihnen zu erkennen, unabhÃ¤ngig von ihrem Abstand in der Sequenz. In Java, wie in jeder anderen Programmiersprache, wÃ¼rde die Implementierung dieser Logik das Erstellen von Matrizen fÃ¼r die Eingaben, Gewichte fÃ¼r die Attention-Scores und anschlieÃŸende Matrixmultiplikationen erfordern, um die gewichtete Summe zu erhalten, die die Ausgabe bildet.