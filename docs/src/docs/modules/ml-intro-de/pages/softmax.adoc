= Softmax

Die Softmax-Funktion ist ein wichtiger Bestandteil vieler neuronaler Netzwerke, insbesondere wenn es um Aufgaben der Klassifizierung geht. Sie wird typischerweise in der letzten Schicht eines Netzwerks verwendet, um die Ausgaben so zu normalisieren, dass sie als Wahrscheinlichkeiten interpretiert werden k√∂nnen. Jeder Ausgabewert liegt zwischen 0 und 1, und die Summe aller Ausgabewerte ist 1. Die Softmax-Funktion wird durch die folgende Gleichung definiert:

[stem]
++++
\begin{equation}
\text{softmax}(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
\end{equation}
++++