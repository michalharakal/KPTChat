<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Untitled :: KPTChat</title>
    <link rel="canonical" href="https://michalharakal.github.io/KPTChat/fj/nanogpt-lecture/code_erkläret.html">
    <meta name="generator" content="Antora 3.1.7">
    <link rel="stylesheet" href="../../_/css/site.css">
    <script>var uiRootPath = '../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://michalharakal.github.io/KPTChat">KPTChat</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="#">Home</a>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Products</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="#">Product A</a>
            <a class="navbar-item" href="#">Product B</a>
            <a class="navbar-item" href="#">Product C</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Services</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="#">Service A</a>
            <a class="navbar-item" href="#">Service B</a>
            <a class="navbar-item" href="#">Service C</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Resources</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="#">Resource A</a>
            <a class="navbar-item" href="#">Resource B</a>
            <a class="navbar-item" href="#">Resource C</a>
          </div>
        </div>
        <div class="navbar-item">
          <span class="control">
            <a class="button is-primary" href="#">Download</a>
          </span>
        </div>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="fj" data-version="">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">KPTChat</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Workshop</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../setup.html">Set up an environment</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../bios.html">Wer wir sind</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">NanoGPT</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="index.html">Welcome</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="analysis.html">Analysis</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="summary.html">Summary</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="sources.html">Links and sources</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">KPTChat</span>
    <span class="version">default</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">KPTChat</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">default</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
</nav>
  <div class="edit-this-page"><a href="https://github.com/michalharakal/KPTChat/docs/src/docs/modules/nanogpt-lecture/pages/code_erkläret.adoc">Edit this Page</a></div>
  </div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<div class="paragraph">
<p>Der bereitgestellte Python-Code definiert und trainiert ein vereinfachtes GPT-ähnliches Sprachmodell mit PyTorch, einer beliebten Deep-Learning-Bibliothek. Das Modell ist darauf ausgelegt, Text zu generieren, indem es das nächste Zeichen in einer Sequenz vorhersagt, basierend auf seinem Verständnis des vorherigen Kontexts. Lassen Sie uns die Schlüsselkomponenten und Schritte dieses Codes aufschlüsseln:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Importe und Hyperparameter</strong>:</p>
<div class="ulist">
<ul>
<li>
<p>Importiert notwendige PyTorch-Module und Funktionen.</p>
</li>
<li>
<p>Setzt Hyperparameter wie Batch-Größe, Blockgröße (maximale Kontextlänge für Vorhersagen), Anzahl der Iterationen für Training und Bewertung, Lernrate und Spezifikationen der Modellarchitektur (Einbettungsgröße, Anzahl der Aufmerksamkeitsköpfe, Schichten und Dropout-Rate).</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Daten-Vorbereitung</strong>:</p>
<div class="ulist">
<ul>
<li>
<p>Liest eine Textdatei ('input.txt') und verarbeitet sie, um ein Vokabular von einzigartigen Zeichen zu erstellen, wobei jedes Zeichen einer einzigartigen Zahl zugeordnet wird (und umgekehrt) für die Verarbeitung durch das neuronale Netzwerk.</p>
</li>
<li>
<p>Teilt die Textdaten in Trainings- und Validierungsdatensätze auf.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Datenlade-Funktion (<code>get_batch</code>)</strong>:</p>
<div class="ulist">
<ul>
<li>
<p>Generiert Batches von Eingabesequenzen (<code>x</code>) und ihren entsprechenden Zielsequenzen (<code>y</code>) für das Training oder die Validierung. Jede Zielsequenz ist die Eingabesequenz, die um ein Zeichen nach rechts verschoben ist, was die Aufgabe der Vorhersage des nächsten Zeichens anzeigt.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Modellkomponenten</strong>:</p>
<div class="ulist">
<ul>
<li>
<p>Definiert mehrere Schlüsselkomponenten des Transformer-Modells, das in GPT verwendet wird:</p>
<div class="ulist">
<ul>
<li>
<p><code>Head</code>: Ein einzelner Aufmerksamkeitskopf.</p>
</li>
<li>
<p><code>MultiHeadAttention</code>: Kombiniert mehrere Aufmerksamkeitsköpfe.</p>
</li>
<li>
<p><code>FeedForward</code>: Ein Feedforward-Neuralnetzwerk, das nach dem Aufmerksamkeitsmechanismus angewendet wird.</p>
</li>
<li>
<p><code>Block</code>: Ein einzelner Transformer-Block, der Selbst-Aufmerksamkeit und Feedforward-Layer kombiniert.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Diese Komponenten werden verwendet, um das <code>GPTLanguageModel</code> zu bauen, das die Einbettungsschichten für Token und Positionen, eine Sequenz von Transformer-Blöcken und eine abschließende Schicht zur Vorhersage des nächsten Zeichens umfasst.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Modelltraining und -bewertung</strong>:</p>
<div class="ulist">
<ul>
<li>
<p>Initialisiert das Modell und verschiebt es auf das entsprechende Gerät (GPU oder CPU).</p>
</li>
<li>
<p>Richtet einen Optimierer für das Training ein.</p>
</li>
<li>
<p>Führt eine Trainingsschleife durch, die periodisch das Modell auf Trainings- und Validierungsdaten bewertet, um den Verlust zu überwachen (unter Verwendung der Funktion <code>estimate_loss</code>).</p>
</li>
<li>
<p>Verwendet Gradientenabstieg, um die Gewichte des Modells basierend auf dem berechneten Verlust zu aktualisieren.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Textgenerierung</strong>:</p>
<div class="ulist">
<ul>
<li>
<p>Nach dem Training kann das Modell Text generieren, indem es wiederholt das nächste Zeichen in einer Sequenz vorhersagt und es dem Eingabekontext anhängt. Dieser Prozess verwendet die Funktion <code>generate</code>, die aus den Vorhersagen des Modells eine Sequenz von Zeichen sampelt.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Hilfsfunktionen</strong>:</p>
<div class="ulist">
<ul>
<li>
<p>Die Funktionen <code>encode</code> und <code>decode</code> werden verwendet, um zwischen Textstrings und Listen von Ganzzahlindizes zu konvertieren, entsprechend dem Vokabular des Modells.</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>Die Architektur und der Trainingsprozess des Modells basieren auf den Prinzipien des Transformer-Modells, speziell angepasst für die zeichenbasierte Sprachmodellierung. Dies umfasst die Verwendung von Selbst-Aufmerksamkeit, um Abhängigkeiten zwischen Zeichen in der Eingabesequenz zu erfassen, Layer-Normalisierung und Dropout zur Regularisierung. Das Ziel ist es, ein Modell zu trainieren, das kohärenten und kontextuell relevanten Text generieren kann, indem es</p>
</div>
<div class="paragraph">
<p>ate).</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Datenvorbereitung</strong>:</p>
<div class="ulist">
<ul>
<li>
<p>Liest eine Textdatei ('input.txt') und verarbeitet sie, um ein Vokabular einzigartiger Zeichen zu erstellen, wobei jedem Zeichen eine eindeutige Zahl zugeordnet wird (und umgekehrt) für die Verarbeitung durch das neuronale Netzwerk.</p>
</li>
<li>
<p>Teilt die Textdaten in Trainings- und Validierungssets auf.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Datenladefunktion (<code>get_batch</code>)</strong>:</p>
<div class="ulist">
<ul>
<li>
<p>Erzeugt Batches von Eingabesequenzen (<code>x</code>) und den entsprechenden Zielsequenzen (<code>y</code>) für das Training oder die Validierung. Jede Zielsequenz ist die Eingabesequenz um ein Zeichen nach rechts verschoben, was die Aufgabe der nächsten Zeichenvorhersage anzeigt.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Modellkomponenten</strong>:</p>
<div class="ulist">
<ul>
<li>
<p>Definiert mehrere Schlüsselkomponenten des Transformators, die im GPT verwendet werden:</p>
<div class="ulist">
<ul>
<li>
<p><code>Head</code>: Ein einzelner Aufmerksamkeitskopf.</p>
</li>
<li>
<p><code>MultiHeadAttention</code>: Kombiniert mehrere Aufmerksamkeitsköpfe.</p>
</li>
<li>
<p><code>FeedForward</code>: Ein vorwärts gerichtetes neuronales Netzwerk, das nach dem Aufmerksamkeitsmechanismus angewendet wird.</p>
</li>
<li>
<p><code>Block</code>: Ein einzelner Transformer-Block, der Selbst-Aufmerksamkeit und Vorwärts-Netzwerke kombiniert.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Diese Komponenten werden verwendet, um das <code>GPTLanguageModel</code> zu bauen, das die Einbettungsschichten für Token und Positionen, eine Sequenz von Transformer-Blöcken und eine abschließende Schicht zur Vorhersage des nächsten Zeichens umfasst.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Modelltraining und -bewertung</strong>:</p>
<div class="ulist">
<ul>
<li>
<p>Initialisiert das Modell und verschiebt es auf das entsprechende Gerät (GPU oder CPU).</p>
</li>
<li>
<p>Richtet einen Optimierer für das Training ein.</p>
</li>
<li>
<p>Führt eine Trainingsschleife durch, die das Modell regelmäßig sowohl auf Trainings- als auch auf Validierungsdaten evaluiert, um den Verlust zu überwachen (mit der Funktion <code>estimate_loss</code>).</p>
</li>
<li>
<p>Verwendet den Gradientenabstieg, um die Gewichte des Modells basierend auf dem berechneten Verlust zu aktualisieren.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Textgenerierung</strong>:</p>
<div class="ulist">
<ul>
<li>
<p>Nach dem Training kann das Modell Text generieren, indem es wiederholt das nächste Zeichen in einer Sequenz vorhersagt und es dem Eingabekontext hinzufügt. Dieser Prozess verwendet die Funktion <code>generate</code>, die aus den Vorhersagen des Modells auswählt, um eine Sequenz von Zeichen zu erzeugen.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Hilfsfunktionen</strong>:</p>
<div class="ulist">
<ul>
<li>
<p>Die Funktionen <code>encode</code> und <code>decode</code> werden verwendet, um zwischen Textstrings und Listen von Ganzzahlen zu konvertieren, entsprechend dem Vokabular des Modells.</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>Die Architektur und der Trainingsprozess des Modells basieren auf den Prinzipien des Transformer-Modells, speziell angepasst für die zeichenbasierte Sprachmodellierung. Dies umfasst die Verwendung von Selbst-Aufmerksamkeit, um Abhängigkeiten zwischen Zeichen in der Eingabesequenz zu erfassen, Schichtnormalisierung und Dropout zur Regularisierung. Ziel ist es, ein Modell zu trainieren, das kohärenten und kontextuell relevanten Text generieren kann, indem es Muster in den Trainingsdaten lernt.</p>
</div>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <p>This page was built using the Antora default UI.</p>
  <p>The source code for this UI is licensed under the terms of the MPL-2.0 license.</p>
</footer>
<script src="../../_/js/site.js"></script>
<script async src="../../_/js/vendor/highlight.js"></script>
  </body>
</html>
